{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/anthonysalvatore/comp-machine-learning-a2/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7eA4q3yYuNRI"
   },
   "source": [
    "# **Machine Learning Assignment 2: Classify Images of Cancer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhfAFZAE4SZh"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Within this assignment, a modified version of the 'CRCHistoPhenotypes' dataset is used to perform two major tasks being:\n",
    "\n",
    "*   Classify cell images according to whether a given cell image represents a cancerous cell or not. \n",
    "*   Classify cell images according to cell-type.\n",
    "\n",
    "Machine learning will be used to develop the classification systems capable of performing the afforementioned tasks. Different techniques will be explored to ultimately arrive at a model capable of performing classification to a degree that is in line with the existing literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rITkBHznueAA"
   },
   "source": [
    "## Setting environment, loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKF_0_zhk7iS",
    "outputId": "e24929f8-cc4c-498a-d007-36dbe9ad16e1"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/anthonysalvatore/comp-machine-learning-a2 # only run in colab\n",
    "IMAGE_FILE_PATH = './Image_classification_data/patch_images/' # change if not in colab\n",
    "DATA_FILE_PATH = 'Image_classification_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qceKhQCMpmaH"
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "nrX0FiS5lAan"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'comp-machine-learning-a2/Image_classification_data/data_labels_mainData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the CSV files\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m main_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomp-machine-learning-a2/Image_classification_data/data_labels_mainData.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mInstanceID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# change path if not in colab\u001b[39;00m\n\u001b[0;32m      3\u001b[0m extra_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomp-machine-learning-a2/Image_classification_data/data_labels_extraData.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstanceID\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'comp-machine-learning-a2/Image_classification_data/data_labels_mainData.csv'"
     ]
    }
   ],
   "source": [
    "# load the CSV files\n",
    "main_data = pd.read_csv(DATA_FILE_PATH + '/data_labels_mainData.csv', index_col='InstanceID') # change path if not in colab\n",
    "extra_data = pd.read_csv(DATA_FILE_PATH + 'comp-machine-learning-a2/Image_classification_data/data_labels_extraData.csv', index_col='InstanceID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lp3npouGumjo"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q82EJuZvzI7R",
    "outputId": "8860c2b6-fbc6-44c7-ddd2-eac275c1bc5e"
   },
   "outputs": [],
   "source": [
    "print(main_data.head())\n",
    "print(extra_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9CZmITQpGPJ",
    "outputId": "49ffec8a-062f-4341-ca9f-f630b9d94561"
   },
   "outputs": [],
   "source": [
    "# display summary statistics of main_data\n",
    "print(main_data.describe())\n",
    "print(extra_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lui1tuFDHeXW",
    "outputId": "dea80c3b-2a0d-44cd-de0d-e0b2265ae847"
   },
   "outputs": [],
   "source": [
    "# confirm variable types and check for null values\n",
    "print(main_data.info())\n",
    "print(extra_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E81kDEifPpai"
   },
   "source": [
    "As shown from the information generated above, we can see that we will be working with mostly numerical data with some objects fields defining the labels of the cells and images. This has also shown that there are no 'null' values in the data, but we will perform additional checks for constant values and null values to confirm what is shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TU1qVijjH-AL",
    "outputId": "628c4811-578c-4237-966f-09400cf88243"
   },
   "outputs": [],
   "source": [
    "# Checking for constant values\n",
    "# Determine if any features of the dataframe have constant value across all observations\n",
    "constant = []\n",
    "for feature in main_data.columns:\n",
    "    if main_data[feature].nunique() == 1:\n",
    "        constant.append(feature)\n",
    "        \n",
    "if len(constant) > 0:\n",
    "    print(\"The main_data dataframe contains constant features.\")\n",
    "    for item in constant:\n",
    "        print(f\"{item} is constant.\")\n",
    "else:\n",
    "    print(\"The main_data dataframe does not contain any constant features.\")\n",
    "\n",
    "# Repeat the same process for the extra data\n",
    "constants = []\n",
    "for feature in extra_data.columns:\n",
    "    if main_data[feature].nunique() == 1:\n",
    "        constants.append(feature)\n",
    "        \n",
    "if len(constants) > 0:\n",
    "    print(\"The extra_data dataframe contains constant features.\")\n",
    "    for items in constants:\n",
    "        print(f\"{items} is constant.\")\n",
    "else:\n",
    "    print(\"The extra_data dataframe does not contain any constant features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K52HCTehII6-",
    "outputId": "f51c3be5-8969-4890-b7f2-562b563ddf3c"
   },
   "outputs": [],
   "source": [
    "# Checking for missin/null values\n",
    "# Confirming results of .info()\n",
    "# Determine if the dataframe contains missing values\n",
    "missing = False\n",
    "for feature in main_data:\n",
    "    for observation in feature:\n",
    "        # We define missing values as None types, empty strings, or the NumPy type NaN\n",
    "        if observation is None or observation == '' or observation == np.NaN:\n",
    "            missing = True\n",
    "if missing == True:\n",
    "    print(\"The main_data dataframe contains missing values.\")\n",
    "else:\n",
    "    print(\"The main_data dataframe does not contain any missing values.\")\n",
    "\n",
    "# Repeat the same process for the extra data\n",
    "missings = False\n",
    "for features in extra_data:\n",
    "    for observations in features:\n",
    "        # We define missing values as None types, empty strings, or the NumPy type NaN\n",
    "        if observations is None or observations == '' or observations == np.NaN:\n",
    "            missings = True\n",
    "if missings == True:\n",
    "    print(\"The extra_data dataframe contains missing values.\")\n",
    "else:\n",
    "    print(\"The extra_data dataframe does not contain any missing values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O4XDEmAFlHM4",
    "outputId": "302bcd4d-bd20-48ef-e62d-3c1843e9c2fe"
   },
   "outputs": [],
   "source": [
    "# visualize the distribution of isCancerous\n",
    "main_data['isCancerous'].value_counts().plot(kind='bar')\n",
    "plt.title('main_data')\n",
    "plt.xlabel('isCancerous')\n",
    "plt.show()\n",
    "\n",
    "extra_data['isCancerous'].value_counts().plot(kind='bar')\n",
    "plt.title('extra_data')\n",
    "plt.xlabel('isCancerous')\n",
    "plt.show()\n",
    "\n",
    "# display a sample image\n",
    "img = Image.open('comp-machine-learning-a2/Image_classification_data/patch_images/1.png')\n",
    "img.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nsmix2wxtifV"
   },
   "source": [
    "The bar charts clearly show that the Main data is relatively evenly distributed. However, the Extra data plot conveys that there is nearly double the non-cancerous (0) cells in the dataframe when compared to those that are labelled cancerous (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrHuD5win1dD",
    "outputId": "35723bcf-ed8e-4a6e-9f0b-1f66bea0dd6f"
   },
   "outputs": [],
   "source": [
    "# visualize the distribution of cellType\n",
    "main_data['cellType'].hist()\n",
    "plt.title('main_data')\n",
    "plt.xlabel('Cell Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2nQMw3sj16F4"
   },
   "source": [
    "The distribution above shows the modal for the main_data sample are of type 2 (epithelial). The keys are as follows:\n",
    "<br>\n",
    "0.   fibroblast\n",
    "1.   inflammatory\n",
    "2.   epithelial\n",
    "3.   others \n",
    "\n",
    "We can now plot the cell type against the isCancerous field using a stacked bar chart, to determine if there is any relationship between the two fields that may help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0v63yW7h2Dg7",
    "outputId": "da8f58e1-952d-4be9-c2dc-607d13c9f0bf"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "main_data.groupby(['cellType', 'isCancerous']).size().unstack().plot(kind='bar', stacked=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oreW-gK491C9"
   },
   "source": [
    "The chart clearly demonstrates that all cells of type 2 are cancerous, while every other cell type (0, 1 & 3) have no cancerous cells present. We can confirm the results through a cross table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "klUS7VmgASnn",
    "outputId": "0fedeb20-3b8b-4db4-9803-6e402aa61b3c"
   },
   "outputs": [],
   "source": [
    "table = pd.crosstab(main_data['cellType'], main_data['isCancerous'])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-eG4o-oAnWY"
   },
   "source": [
    "The cross table echoes the findings of the bar chart, that all cancerous cells belong to the epitherial cell type (2). While epitherial cells are the most common type of cell in which cancers develop [1], they are obivoulsy not all cancerous cells in a real-life setting. Despite this, we will be proceeding with the assumption that every cell of type 2 is cancerous as this is how the data has been presented in this case study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz8Mkrg9Hk4m"
   },
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UU763uyYHnU-"
   },
   "source": [
    "From our EDA, we have discovered that accuracy is adequate for the task of classifying a cell as cancerous or not. This is supported by the even distribtuion of classes and the fact that it is a binary class. However, since it is often more valuable to know if a cell is cancerous rather than not in a real-world setting, the recall could also be used, to minimize the number of false negatives.\n",
    "<br>\n",
    "For the task of cell type classification, since there are multiple classes to predict, accuracy may not be the best metric to use. Instead, the F1-score can be used, as it provides a good expression of precision and recall and is more robust than accuracy when multiple classes are involved. It is especially useful here since our cellType field will likely be much more unevenly distributed.\n",
    "<br><br>\n",
    "[pull relevant accuracy scores from literature, refer to here and set as target]\n",
    "<br><br>\n",
    "From the EDA, we discovered that every cell which was epitherial was cancerous. Knowing this, we can assign a label of '2' in the cellType column for each cancerous cell in the extra data dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LkNs906z8Muz",
    "outputId": "923283b1-3ba7-42be-d04f-ed795d7cc9e9"
   },
   "outputs": [],
   "source": [
    "extra_data.loc[extra_data['isCancerous'] == 1, 'cellType'] = 2\n",
    "extra_data.loc[extra_data['isCancerous'] == 1, 'cellTypeName'] = 'epitherial'\n",
    "extra_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjruhY-5IV9Y"
   },
   "source": [
    "After applying the change to the extra data dataframe, we can merge it with the main data dataframe. Using both the main and extra data is well justified in the case of classifying isCancerous since both contain the full data needed. The extra data dataframe can supplement the main data in the case of cellType classification, as it now contains some data pertaining to cellType. However, since we were only able to discern epitherial cells from our EDA, the data is likely to be heavily skewed towards cellType 2 and may cause the classifier to have more of a bias for classifying cells as belonging to cell type 2. This will be explored further throughout the report. After merging the dataframes, a distribution of cell types can be seen in the graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCySIqGbJ8UT",
    "outputId": "da5d8b6d-2f69-474c-d19f-93ec9c6ac610"
   },
   "outputs": [],
   "source": [
    "df = pd.concat([main_data, extra_data]).sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAbzA9H4Lnbw",
    "outputId": "92b256eb-a24c-40c5-dc73-c9ab9739d734"
   },
   "outputs": [],
   "source": [
    "df['cellType'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDekb2LRMovh"
   },
   "source": [
    "As expected, imputing a value of 2 for each cellType in the extra data dataframe that contained a cancerous cell has further inbalanced the classes within this field.\n",
    "<br><br>\n",
    "We will now create two datasets off of the merged dataframe to perform each of the two classification tasks required. This is necessary since we have missing values in our cellType and cellTypeName column so we need to distinguish between the two before we perform a split. We also drop all unnecessary columns here for each dataframe. Only the labels and image name are needed for model training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TA3whIC2RXm_"
   },
   "outputs": [],
   "source": [
    "df_cancerous = df.drop(['patientID', 'cellTypeName', 'cellType'], axis=1)\n",
    "df_type = df.dropna().drop(['patientID', 'cellTypeName', 'isCancerous'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q65WsPEVNqlY",
    "outputId": "e254d031-7b39-49cb-aea7-73d0a669e658"
   },
   "outputs": [],
   "source": [
    "print(df_cancerous)\n",
    "print(df_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUg423wUTxk2"
   },
   "source": [
    "We now have a clean dataframe for each classification task, containing no missing values.\n",
    "<br><br>\n",
    "Next, we split each dataframe into test, validation and training sets to be used for model training, validation and testing respectively. Here we choose a standard 60/20/20 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CgeznmwjUxUn",
    "outputId": "267d7943-f5a7-4d42-cb36-65327f2003c3"
   },
   "outputs": [],
   "source": [
    "# train/val/test split for both dataframes\n",
    "train_data_c, test_data_c = train_test_split(df_cancerous, test_size=0.2, random_state=42)\n",
    "train_data_c, val_data_c = train_test_split(train_data_c, test_size=0.25, random_state=42)\n",
    "\n",
    "train_data_t, test_data_t = train_test_split(df_type, test_size=0.2, random_state=42)\n",
    "train_data_t, val_data_t = train_test_split(train_data_t, test_size=0.25, random_state=42)\n",
    "\n",
    "print(\"Cancerous train data : {}, Cancerous val data: {}, Cancerous test data: {}\".format(train_data_c.shape[0], val_data_c.shape[0], test_data_c.shape[0]))\n",
    "print(\"Cell type train data : {}, Cell type val data: {}, Cell type test data: {}\".format(train_data_t.shape[0], val_data_t.shape[0], test_data_t.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VKLRzc4GVQS_",
    "outputId": "c4f46fa7-1931-4fd2-bf71-0708082e5adc"
   },
   "outputs": [],
   "source": [
    "train_data_c.hist()\n",
    "val_data_c.hist()\n",
    "test_data_c.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Utw9j4tZV8MB",
    "outputId": "cb7e2a6e-d07c-43a9-f94b-904499f32644"
   },
   "outputs": [],
   "source": [
    "train_data_t.hist()\n",
    "val_data_t.hist()\n",
    "test_data_t.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_0CwspEWFrC"
   },
   "source": [
    "Distribution appears fairly similar between train/test/val for all pairings.\n",
    "<br>\n",
    "Since this is an image classification problem, a neural network works very well on the non-linear and huge number of parameters of an image. Since we intend to use neural networks for this project, a method of evaluating the rate of learning of the network is necessary so we can decide on next-step actions. We can plot the learning curve of the NN training process using the function below. This function will serve as our diagnostic tool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "46LojeLadbYm"
   },
   "outputs": [],
   "source": [
    "# code taken from [2]\n",
    "def plot_learning_curve(train_loss, val_loss, train_metric, val_metric, metric_name='Accuracy'):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_loss, 'r--')\n",
    "    plt.plot(val_loss, 'b--')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(train_metric, 'r--')\n",
    "    plt.plot(val_metric, 'b--')\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHbhzh8BdqYf"
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UVGGMLxydvZk"
   },
   "source": [
    "We require a baseline model as our initial solution. We will iterate over this model throughout remainder of the project, exploring different avenues that can lead to improvements. We will initially use an multi-layer perceptron (MLP) model and later promote this basic model to the more complex convolutional neural network (CNN).\n",
    "<br><br>\n",
    "We need to set some initial parameters for our NN, pertaining to the input dimension, hidden layer dimension and number of classes to output. We will define these parameters below for both of our two classification problems. The hidden layer hyper parameter can be tuned later but will be set at 256 for both initially. 256 represents the value each RGB chanel can range from, so is a fair initial estimate here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k312xFVjhs07"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM_c = (27,27,3)\n",
    "HIDDEN_LAYER_DIM_c = 256\n",
    "OUTPUT_CLASSES_c = 2\n",
    "\n",
    "INPUT_DIM_t = (27,27,3)\n",
    "HIDDEN_LAYER_DIM_t = 256\n",
    "OUTPUT_CLASSES_t = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMh98wJ9iSxv"
   },
   "source": [
    "With the above parameters defined, we can begin to build out the model. For this, we will use the popular machine learning library tensorflow, specifically the API keras which provides a Python interface for artificial neural networks. We work in layers, building our neural network up in a number of interconnected layers, defined by our set parameters.\n",
    "<br> We use the sequential API as it is the simplest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7-sW1rQjLCa"
   },
   "outputs": [],
   "source": [
    "model_c = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=INPUT_DIM_c),\n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_DIM_c),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES_c)\n",
    "])\n",
    "\n",
    "model_t = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=INPUT_DIM_t),\n",
    "    tf.keras.layers.Dense(HIDDEN_LAYER_DIM_t),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES_t)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-K7Vdbpj5qk"
   },
   "source": [
    "The first layer transforms the images from a two dimensional image array, to a one dimensional array (27 * 27 * 3 = 2187 px). The next two layers are densely connected neural layers, with the last being related to the class to be classified for each problem.\n",
    "<br><br>\n",
    "model.summary() can print a brief summary of the model that we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vCsgALM9kiX8",
    "outputId": "aabfb662-ea19-4df6-e8f9-1beeb01e050e"
   },
   "outputs": [],
   "source": [
    "model_c.summary()\n",
    "model_t.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J773U9vElTsr"
   },
   "source": [
    "tf.keras.utils.plot_model can present the model as a figuire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "W6AeHYJklMJS",
    "outputId": "513816e5-0b68-4a3c-ce58-e41e15a23082"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_c, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "JMw1HHF6AQqe",
    "outputId": "406fc0e4-8f2f-4937-acf7-f2cbf94c0042"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_t, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6P5kWdAdA9UL"
   },
   "source": [
    "The model requires a few other parameters before it is ready for training. These are included during the compile step as seen below. We set the loss function, optimizer and metrics. As detailed before, we will use accuracy for the cancerous classification task, and F1 score for the cell type classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRUzdLgDdUbw"
   },
   "outputs": [],
   "source": [
    "# define F1 score metric \n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "    true_positives = tf.math.reduce_sum(tf.math.multiply(y_true, y_pred), axis=0)\n",
    "    false_positives = tf.math.reduce_sum(tf.math.multiply(1 - y_true, y_pred), axis=0)\n",
    "    false_negatives = tf.math.reduce_sum(tf.math.multiply(y_true, 1 - y_pred), axis=0)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives + 1e-8)\n",
    "    recall = true_positives / (true_positives + false_negatives + 1e-8)\n",
    "\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "    return tf.reduce_mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lQqXWGdAbk7"
   },
   "outputs": [],
   "source": [
    "# compile models\n",
    "model_c.compile(optimizer='SGD',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['categorical_accuracy'])\n",
    "\n",
    "model_t.compile(optimizer='SGD',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM3C_fLQCYbI"
   },
   "source": [
    "When training the model, it is important to load the data to memory in batches, since our data will be large and complex. We achieve this in keras via image data generators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fIcbyZgjDHw3"
   },
   "outputs": [],
   "source": [
    "# convert class labels to string - required for proceeding function\n",
    "train_data_c['isCancerous'] = train_data_c['isCancerous'].astype('str')\n",
    "val_data_c['isCancerous'] = val_data_c['isCancerous'].astype('str')\n",
    "test_data_c['isCancerous'] = test_data_c['isCancerous'].astype('str')\n",
    "\n",
    "train_data_t['cellType'] = train_data_t['cellType'].astype('str')\n",
    "val_data_t['cellType'] = val_data_t['cellType'].astype('str')\n",
    "test_data_t['cellType'] = test_data_t['cellType'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4QwiYe5tDkDs",
    "outputId": "a69356ea-1724-4e31-f635-533b807550cb"
   },
   "outputs": [],
   "source": [
    "# isCancerous classification task\n",
    "train_datagen_c = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "val_datagen_c = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "test_datagen_c = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator_c = train_datagen_c.flow_from_dataframe(\n",
    "        dataframe=train_data_c,\n",
    "        directory=IMAGE_FILE_PATH, # change if not in colab\n",
    "        x_col=\"ImageName\",\n",
    "        y_col=\"isCancerous\",\n",
    "        target_size=(27, 27),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator_c = val_datagen_c.flow_from_dataframe(\n",
    "        dataframe=val_data_c,\n",
    "        directory=IMAGE_FILE_PATH, # change if not in colab\n",
    "        x_col=\"ImageName\",\n",
    "        y_col=\"isCancerous\",\n",
    "        target_size=(27, 27),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_generator_c = test_datagen_c.flow_from_dataframe(\n",
    "        dataframe=test_data_c,\n",
    "        directory=IMAGE_FILE_PATH, # change if not in colab\n",
    "        x_col=\"ImageName\",\n",
    "        y_col=\"isCancerous\",\n",
    "        target_size=(27, 27),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qMUsS6xaEyg5",
    "outputId": "e2472fae-134f-44dc-a844-5f2c92a8e089"
   },
   "outputs": [],
   "source": [
    "# cellType classification task\n",
    "train_datagen_t = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "val_datagen_t = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "test_datagen_t = ImageDataGenerator(rescale=1./255, data_format='channels_last')\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator_t = train_datagen_t.flow_from_dataframe(\n",
    "        dataframe=train_data_t,\n",
    "        directory=IMAGE_FILE_PATH, # change if not in colab\n",
    "        x_col=\"ImageName\",\n",
    "        y_col=\"cellType\",\n",
    "        target_size=(27, 27),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "validation_generator_t = val_datagen_t.flow_from_dataframe(\n",
    "        dataframe=val_data_t,\n",
    "        directory=IMAGE_FILE_PATH, # change if not in colab\n",
    "        x_col=\"ImageName\",\n",
    "        y_col=\"cellType\",\n",
    "        target_size=(27, 27),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')\n",
    "\n",
    "test_generator_t = test_datagen_t.flow_from_dataframe(\n",
    "        dataframe=test_data_t,\n",
    "        directory=IMAGE_FILE_PATH, # change if not in colab\n",
    "        x_col=\"ImageName\",\n",
    "        y_col=\"cellType\",\n",
    "        target_size=(27, 27),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9Pb3mAgGfsF"
   },
   "source": [
    "In the above, we've defined an image data generator which will load each of our images into memory in batches, based off the provided directory and file name given in our dataframe. We also normalize each of the pixels from their regular scale of 0-255.\n",
    "<br><br>\n",
    "With all of that out of the way, we are now able fit our models to the data, and observe the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GET6q9n9HBWz",
    "outputId": "67e8a0e4-e5a8-4f51-c9bc-86f4c1ae1217"
   },
   "outputs": [],
   "source": [
    "history_c = model_c.fit_generator(train_generator_c, validation_data = validation_generator_c, epochs=50, verbose=0)\n",
    "history_t = model_t.fit_generator(train_generator_t, validation_data = validation_generator_t, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "x-R8vKFeHZQ_",
    "outputId": "6354b7ee-9510-4444-d4f5-f81566e10b66"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(history_c.history['loss'], history_c.history['val_loss'], \n",
    "                    history_c.history['categorical_accuracy'], history_c.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "ZJuJe8zqHhJk",
    "outputId": "9fa57ef6-cd1b-4119-8956-7fb02ca631b2"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(history_t.history['loss'], history_t.history['val_loss'], \n",
    "                    history_t.history['categorical_accuracy'], history_t.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNRvfiGhIQJ-",
    "outputId": "201b8431-679f-4d7e-f209-005280cfb91c"
   },
   "outputs": [],
   "source": [
    "test_loss_c, test_acc_c = model_c.evaluate(test_generator_c, verbose=0)\n",
    "print('Test loss:', test_loss_c)\n",
    "print('Test accuracy:', test_acc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EUCmkvI9PisG",
    "outputId": "6336845f-4ad0-4d49-ed8a-b173e685d24b"
   },
   "outputs": [],
   "source": [
    "test_loss_t, test_f1_t = model_t.evaluate(test_generator_t, verbose=0)\n",
    "print('Test loss:', test_loss_t)\n",
    "print('Test f1 score:', test_f1_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4O0Jh7Lp-ZY"
   },
   "source": [
    "The learning curves above demonstrates a decent predictive rate for both our classification tasks. The models generalise well to the unseen test data, performing as good as the validation set. They do not appear to be overfitting, as the training curve and validation curve appear quite close.\n",
    "<br>The task of classifying a cell as cancerous or not is an especially strong predictor. This is to be expected, given what we know about the dataset and the classification task for each. Predicting isCancerous is a binary problem, with a complete dataset. Predicting the cellType invovles prediction of four labels, with an incomplete dataset. The learner involved in predicting the isCancerous class should always perform better than that of the cellType predictor. <br>However, there are many iterative improvements we can make to both predictors to further bolster their preditive power and have them fall in line closer with the expected result from the literature. One such is testing different base models, to determine which is more appropriate for this task. \n",
    "<br>We will now implement a CNN base model, compare the results to our MLP base model and make an educated judgement about which model to further make imporvements on. Much of the steps taken before for the MLP model are the same, since CNN and MLP methods share much of the same foundation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0RD4z5CpqYxK"
   },
   "source": [
    "We need to select A base model for the CNN task. An IaNet model should be sufficient here, as they are a simple model that can operate on small images. Instead of adding layers as in the MLP model, we add blocks, which contain a convolution, activation and pooling layer. The pooling layer causes a downscaling of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk_Gda1dpL7U"
   },
   "outputs": [],
   "source": [
    "reg_lambda = 0.0001\n",
    "# isCancerous CNN model\n",
    "model_leNet_c = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(27, 27, 3)),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,0], -1, name=None)),\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES_c, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))\n",
    "])\n",
    "\n",
    "# cellType CNN model\n",
    "model_leNet_t = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(27, 27, 3)),\n",
    "    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x[:,:,:,0], -1, name=None)),\n",
    "    \n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), kernel_regularizer=tf.keras.regularizers.l2(reg_lambda)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.MaxPool2D(pool_size=(2,2)),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    \n",
    "    tf.keras.layers.Dense(64),\n",
    "    tf.keras.layers.Activation('relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(OUTPUT_CLASSES_t, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(reg_lambda))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mB93jfzBwJOt"
   },
   "outputs": [],
   "source": [
    "model_leNet_c.compile(optimizer='SGD',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['categorical_accuracy'])\n",
    "\n",
    "model_leNet_t.compile(optimizer='SGD',\n",
    "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbvf-ayrrBgz",
    "outputId": "e4760bd4-4d6f-44aa-acca-4620dcdce2d5"
   },
   "outputs": [],
   "source": [
    "history_c = model_leNet_c.fit_generator(train_generator_c, validation_data = validation_generator_c, epochs=50, verbose=0)\n",
    "history_t = model_leNet_t.fit_generator(train_generator_t, validation_data = validation_generator_t, epochs=50, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "9omeq4EpsEm_",
    "outputId": "916f4f48-0d09-47ee-aa48-a2339d5e18f2"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(history_c.history['loss'], history_c.history['val_loss'], \n",
    "                    history_c.history['categorical_accuracy'], history_c.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "jahPLvrCsIi8",
    "outputId": "cb46ac36-979a-4377-ab98-d76041e93812"
   },
   "outputs": [],
   "source": [
    "plot_learning_curve(history_t.history['loss'], history_t.history['val_loss'], \n",
    "                    history_t.history['categorical_accuracy'], history_t.history['val_categorical_accuracy'], \n",
    "                    metric_name='Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ymDxuHKBsKNL",
    "outputId": "a3713bd4-5566-43e2-aad1-711e3ee193ca"
   },
   "outputs": [],
   "source": [
    "test_loss_c, test_acc_c = model_leNet_c.evaluate(test_generator_c, verbose=0)\n",
    "print('Test loss:', test_loss_c)\n",
    "print('Test accuracy:', test_acc_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgA6YpWuseui",
    "outputId": "e808423d-e7b6-43aa-af2a-493e59acf69c"
   },
   "outputs": [],
   "source": [
    "test_loss_t, test_f1_t = model_leNet_t.evaluate(test_generator_t, verbose=0)\n",
    "print('Test loss:', test_loss_t)\n",
    "print('Test f1 score:', test_f1_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNz4DkI17w5O"
   },
   "source": [
    "From the results above, we can see that the task of classifying a cell as cancerous sees a test accuraccy increase of about 5%, and a decrease in test loss of about 0.7. The task of classifying cellType remains largely unchanged in terms of performance, but shows a very slight increase. It is clear that using a CNN is likely more suited to this task than using an MLP. The models both don't appear to be overfitting either, as the learning gap between the training set and validation set is quite small. The models also generalize well to unseen data as the test sets have performances close to that of the validaiton set.\n",
    "<br><br>\n",
    "For the remainder of this assignment we will be proceeding with a CNN model and making iterative imporovements to explore different options that may result in increased performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GCMFhk3-kPO"
   },
   "source": [
    "## References\n",
    "\n",
    "[1] https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3927155/#:~:text=Epithelial%20tissue%20is%20also%20the,percent%20of%20all%20human%20cancers.<br>\n",
    "[2] [link course notes]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lp3npouGumjo",
    "Qz8Mkrg9Hk4m"
   ],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
